---
title: "Prediction Assignment Writeup"
author: "Al Kepner"
date: "3/29/2021"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(caret)
```

# Reading the data

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. 

```{r reading_data}
library(dplyr)
library(caret)
training_raw <- read.csv("data/pml-training.csv")
testing_raw <- read.csv("data/pml-testing.csv")
raw <- training_raw

```


We note that we have a lot of columns (160) from which to choose features for prediction.

Some of the columns contain a lot of NA values. Some contain mostly blanks or garbage.

For example:
```{r sample_column}
summary(raw$amplitude_yaw_forearm)
```

Many or most of the columns that are classed as factors by default are numeric data with some corrupt or missing values not marked as NA.
  
As a starting point we can look at a subset of the columns that contain only numbers with no missing values.


## Summarize column types and completeness of data by column.
```{r summarize_columns}
class <- sapply(raw, function(x) class(x))
is_na <- sapply(raw, function(x) any(is.na(x)))
col_desc <- data.frame(name = names(raw), 
                       index = seq_along(names(raw)), 
                       class = class, 
                       no_missing = !is_na)
head(col_desc)
```

## Select subsets of columns based on no missing data and integer or numeric content
```{r select_subsets)}
c <- col_desc
no_missing <- col_desc[c$no_missing,]
int_only <- no_missing[no_missing$class=="integer",]
head(int_only)
numeric_only <- no_missing[no_missing$class=="numeric",]
head(numeric_only)
```

So we have identified which columns contain only numeric or integer data with
no missing values.

The first 4 integer columns x through num_window appear to
be record keeping rather that physical measurements of activity.
So we will exclude these 4.
```{r exclude_record_keeping}
int_only <- int_only[-1:-4,]
```

That leaves us with 25 integer columns as candidate features
and 27 numeric columns as candidate features. All of these columns have
no missing data.

Since there are many predictor variables based on physical measurements, we can
try alternate models based on:

    1. 25 integer variables only
    2. 27 numeric variables only
    3. 52 numeric and integer variables

Training with cross validation and measuring in-sample accuracy should help us choose between
these alternate models.

We divide the original raw_training data into training and testing sets:
```{r partition_the_data}
set.seed(12345)
inTrain <- createDataPartition(raw$classe, p=0.70, list=FALSE)
training <- raw[inTrain,]
testing <- raw[-inTrain,]
dim(training)
dim(testing)
```


Now we want to select feature subsets of the testing and training data

```{r subset_features}
training.integer = training[, int_only$index]
testing.integer = testing[, int_only$index]
training.numeric = training[, numeric_only$index]
testing.numeric = testing[, numeric_only$index]
all_numbers = c(int_only$index, numeric_only$index)
training.all = training[, all_numbers]
testing.all = testing[, all_numbers]
dim(training.all)
dim(testing.all)
```


Add the outcome variable classe back into the training data frames:
```{r add_outcome_variable}
training.integer <- training.integer %>% mutate(classe = training$classe)
training.numeric <- training.numeric %>% mutate(classe = training$classe)
training.all <- training.all %>% mutate(classe = training$classe)
```


## Try fitting each model with random forest
```{r fit_integer_features, cache=TRUE }
set.seed(2468)
old <- Sys.time()
integer.fit <- train(classe ~ ., 
                data=training.integer,
                method = 'rf',
                trControl = trainControl(method='cv',
                                         number=5))
integer_fit_time <- Sys.time() - old
```

```{r}
print(integer_fit_time)
print(integer.fit)
```

```{r fit_numeric_features, cache=TRUE }
set.seed(2468)
old <- Sys.time()
numeric.fit <- train(classe ~ ., 
                data=training.numeric,
                method = 'rf',
                trControl = trainControl(method='cv',
                                         number=5))
numeric_fit_time <- Sys.time() - old
```

```{r}
print(numeric_fit_time)
print(numeric.fit)
```


```{r fit_all_features, cache=TRUE }
set.seed(2468)
old <- Sys.time()
all.fit <- train(classe ~ ., 
                data=training.all,
                method = 'rf',
                trControl = trainControl(method='cv',
                                         number=5))
all_fit_time <- Sys.time() - old
```

```{r}
print(all_fit_time)
print(all.fit)
```

```{r show_table, echo=FALSE, results="asis"}

i <- integer.fit$results
i_acc <- i[1,2]
i_acc

n <- numeric.fit$results
n_acc <- n[2,2]
n_acc

a <- all.fit$results
a_acc <- a[2,2]
a_acc
Predictors_Used <- c("25 integer columns", "27 numeric columns", "52 integer + numeric columns")
Accuracy <- c(i_acc, n_acc, a_acc)
Traning_Time <- c(integer_fit_time, numeric_fit_time, all_fit_time)
SummaryFrame <- data.frame(Predictors_Used=Predictors_Used, In_Sample_Accuracy=Accuracy, Traning_Time)
library(xtable)
xt <- xtable(SummaryFrame, digits=4)
print(xt, type="html")
```

The random forest model trained with all 52 numeric and integer predictors has the best in-sample accuracy of 99.14 %, but was only slightly better than a random forest trained on only the 27 numeric columns. For a larger data set we might prefer the model using only 27 predictors, Since the training time was approximately doubled with 52 predictors.

## Next assess the out-of-sample accuracy for the 3 models.

We have reserved about 30% of the observations for testing the models.
We can use each of the fitted models to predict values for the reserved testing data,
and count the number of predictions that match the classe column.

```{r prediction_with_testing_data}

integer.predict <- predict(integer.fit, newdata = testing.integer)

integer.result <- data.frame(predicted = integer.predict, actual = testing$classe)

integer.result <- integer.result %>% mutate(correct = predicted==actual)

integer.accuracy <- mean(integer.result$correct)
integer.accuracy


numeric.predict <- predict(numeric.fit, newdata = testing.numeric)

numeric.result <- data.frame(predicted = numeric.predict, actual = testing$classe)

numeric.result <- numeric.result %>% mutate(correct = predicted==actual)

numeric.accuracy <- mean(numeric.result$correct)
numeric.accuracy


all.predict <- predict(all.fit, newdata = testing.all)

all.result <- data.frame(predicted = all.predict, actual = testing$classe)

all.result <- all.result %>% mutate(correct = predicted==actual)

all.accuracy <- mean(all.result$correct)
all.accuracy
```

```{r final_table, echo=FALSE, results="asis"}

out_accuracy <- c(integer.accuracy, numeric.accuracy, all.accuracy)
SummaryFrame <- SummaryFrame %>% mutate(Out_Of_Sample_Accuracy = out_accuracy)
library(xtable)
xt <- xtable(SummaryFrame, digits=4)
print(xt, type="html")
```

## Conclusions
The random forest prediction algorithm works very well for this data set.

Based on both the run time and the estimated out-of-sample prediction accuracy, we think the model using the 27 numeric columns is preferred for this data.

